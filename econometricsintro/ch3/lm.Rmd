---
title: 线性回归分析
author:
 - 授课教师：吴翔 \newline
 - 邮箱：wuhsiang@hust.edu.cn
date: "March 16, 2019"
linestretch: 1.25
fontsize: 18
header-includes:
  - \usepackage{ctex}
output:
  beamer_presentation:
    theme: "CambridgeUS"
    colortheme: "beaver"
    latex_engine: xelatex
    toc: true
---

```{r setup, include = FALSE}

knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
options(digits = 2)

```


# 线性回归概述

## 简单案例

考虑智力测验成绩$x$、教育年限$z$和年收入$y$（万元）之间的关系。数据生成过程（**data generating process**, DGP） $y = -0.5 + 0.2 \cdot x$得到的样本。

```{r}

# clear directory
rm(list = ls())
set.seed(123)

```


```{r, echo = TRUE}

# generate dataset
x <- rnorm(n = 200, mean = 110, sd = 10)
beta <- c(-0.5, 0.2)
y <- beta[1] + beta[2] * x + rnorm(n = 200, mean = 0, sd = 0.5)
z <- round(-2 + 0.1 * x + rnorm(n = 200, mean = 0, sd = 0.4))
dat <- data.frame(x = x, y = y, z = z)

```

## 回归分析

```{r}

# linear regression
fit1 <- lm(y ~ x, data = dat)
summary(fit1)$coef

```

考虑$x$对$y$的效应，线性模型$R^{2} = `r summary(fit1)$r.squared`$，预测值$\hat{\beta} = (`r fit1$coef[1]`, `r fit1$coef[2]`)$接近实际值$\beta = (-0.5, 0.2)$。

```{r}

fit2 <- lm(y ~ z, data = dat)
summary(fit2)$coef

```

考虑$z$对$y$的效应，$y = `r fit2$coef[1]` + `r fit2$coef[2]` z$，且$R^{2} = `r summary(fit2)$r.squared`$。


## 虚假 vs 真实效应

```{r, echo = TRUE}

# linear regression
fit3 <- lm(y ~ x + z, data = dat)
summary(fit3)$coef

```

考虑模型$y = \beta_{0} + \beta_{1} x + \beta_{2} z$。结果显示，$y = `r fit3$coef[1]` + `r fit3$coef[2]` x$，且$R^{2} = `r summary(fit3)$r.squared`$。

**课堂思考: z对y的效应，是否显著？**


## 正效应 vs 负效应？

```{r, echo = TRUE}

# add a sample
dat1 <- rbind(dat, c(160, -100, 10))
fit4 <- lm(y ~ x, data = dat1)
summary(fit4)$coef

```

增加一个样本$c(160, -100, 10)$，重新考虑$x$对$y$的效应，$R^{2} = `r summary(fit4)$r.squared`$，预测值$\hat{\beta} = (`r fit4$coef[1]`, `r fit4$coef[2]`)$大幅偏离实际值$\beta = (-0.5, 0.2)$。

**课堂思考: x对y的效应，到底是正还是负？**

## 如何学习线性回归？


![Master & PhD students who are learning regression models](figures/confused.jpg){width=40%}

**理念**：

- 方便有多门，归元无二路
- 挽弓当挽强，用箭当用长

## 课程存储地址

- 课程存储地址： [https://github.com/wuhsiang/Courses](https://github.com/wuhsiang/Courses)
- 资源：课件、案例数据及代码

![课程存储地址](../../QR.png){width=40%}

## 参考教材

- 谢宇. 回归分析. 北京：社会科学文献出版社. 2010.
- 威廉·贝里. 理解回归假设. 上海:格致出版社. 2012.
- 欧文·琼斯. R语言的科学编程与仿真. 西安：西安交通大学出版社. 2014.


# 线性回归原理

## 缘起

变异与个体差异

- 随着物种的变异，其个体差异是否会一直增大？
- 个体差异上的**两极分化**是否是一般规律？

## Galton的身高研究

```{r, fig.align='center', fig.height=3, fig.width=3}

rm(list = ls())
# load Galton's height data
suppressMessages(library(UsingR))
suppressMessages(library(ggplot2))
data("galton")
# scatter plot
ggplot(galton, aes(x = parent, y = child)) + geom_point() + geom_smooth(method=lm, color="darkred", fill="blue") + theme_bw() + xlim(60, 75) + ylim(60, 75) + geom_abline(intercept = 0, slope = 1)

```

## 什么是“回归”？

Galton的身高研究发现：

- 父代的身高增加时，子代的身高也倾向于增加
- 当父代高于平均身高时，子代身高比他更高的概率要小于比他更矮的概率；父代矮于平均身高时，子代身高比他更矮的概率要小于比他更高的概率。
- 同一族群中，子代的身高通常介于其父代的身高和族群的平均身高之间。

**回归效应**：

- 向平均数方向的回归 (regression toward mediocrity)
- 天之道，损有余而补不足

## Galton的开创性研究

Francis Galton（以及Karl Pearson）研究

- 个体差异：确立了社会科学研究与自然科学研究的根本区别
- 遗传与个体差异的关系：倡导“优生学”
- 双生儿法（twin method）：匹配方法（matching）之先河

## 自然科学 vs 社会科学研究

## 理解回归的三种视角

回归模型考虑解释变量$x$与结果变量$y$的关系，
$$
y_{i} = f(X_{i}) + \epsilon_{i} = \beta X_{i} + \epsilon_{i}
$$
将观测值$y_{i}$分为结构部分$f(X_{i})$和随机部分$\epsilon_{i}$，并可以从**三个视角**来理解：

- **因果性**（计量经济领域）：观测项 = 机制项 + 干扰项
- **预测性**（机器学习领域）：观测项 = 预测项 + 误差项
- **描述性**（统计领域）：观测项 = 概括项 + 残差项

## 回归模型设定

考虑收入$x$与中老年人抑郁水平$y$的关系，回归模型为：
$$
y_{i} = \alpha + \beta x_{i} + \epsilon_{i}.
$$

**暗含的假设**：

- A1. 线性假设（$E(y|x) = \beta x$）：非线性模型、结构模型
- A2. 同质性假设：随机参数/效应模型、分层线性模型


## 总体回归方程

给定$x = x^{k}$，在的$\epsilon_{i} \text{ i.i.d } \sim N(0, \sigma^{2})$假定下，对回归模型求条件期望得到如下**总体回归方程**，

$$
E(y|x = x^{k}) = \mu_{y|x^{k}} = \alpha + \beta x^{k}.
$$

含义：

- 给定任意$x^{k}$，对应的$y^{k} \sim N(\mu_{y|x^{k}}, \sigma^{2})$。
- 回归线穿过$(x^{k},  \mu_{y|x^{k}})$。
- 参数$\beta$刻画了$x$的变化对$y$的**条件期望**的影响。

## 总体回归线

![总体回归线](figures/condmean.png){width=60%}


## 暗含的假设

- A3. 独立同分布假设：
    - $E(\epsilon_{i}) = 0$：随机效应模型中的随机截距参数
    - $Cov(\epsilon_{i}, \epsilon_{j}) = 0$：时间序列模型、空间计量模型、嵌套模型
    - $\sigma_{i} = \sigma$：异方差问题
- A4. 关于$y$的假设：
    - $y$应是连续变量：广义线性模型
    - $y$的条件期望$\mu_{y|x^{k}} = E(y|x = x^{k})$符合正态分布：分位数回归
- A5. 正交（严格外生）假设
    - 误差项$\epsilon$和$x$不相关，即$Cov(x, \epsilon) = 0$
    - 内生性问题

## 参数估计

普通最小二乘法（ordinary least squares, OLS）通过最小化残差平方和（扩展到多元回归的情境$y = \beta X + \epsilon$）估计参数：
$$
\text{min } SSE = \text{min} \sum_{i=1}^{n} (y_{i} - \hat{y}_{i})^{2} = \sum_{i=1}^{n} (y_{i} - \beta X_{i})^{2}
$$
由偏导公式
$$
\frac{\partial SSE}{\partial \beta} = 0
$$
得到参数估计值
$$
\hat{\beta} = (X'X)^{-1}X'y.
$$

**课堂思考: (1)如何在熟悉的编程语言中，撰写函数估计多元线性模型？(2) 在实践中，OLS会造成什么缺陷？**

## 变异分解逻辑

样本观测值$y_{i}$、均值$\bar{y}$、预测值$\hat{y}$之间的关系

![变异的分解](figures/variancedecomp.png){width=60%}

**板书演示：变异分解逻辑**

## 变异分解公式

总平方和（sum of squares total, SST）可以分解为回归平方和（sum of squares regression, SSR）和残差平方和（sum of squares error, SSE）之和，

具体而言：
$$
\begin{aligned}
SST & = \sum_{i=1}^{n}(y_{i} - \bar{y})^{2} \\
    & = \sum_{i=1}^{n} [(y_{i} - \hat{y}_{i}) + (\hat{y}_{i} - \bar{y})]^{2} \\
    & = \sum_{i=1}^{n} (y_{i} - \hat{y}_{i})^{2} + \sum_{i=1}^{n} (\hat{y}_{i} - \bar{y})^{2} \\
    & = SSE + SSR
\end{aligned}
$$

判定系数（coefficient of determination）$R^{2} = SSE / SST.$

**板书演示：变异分解推导**

## 多元线性回归与方差分析

假定多元线性模型中，待估计的参数个数为$p$，那么方差和自由度的分解如下：

- SST: 自由度为$n-1$
- SSE: 自由度为$n - p$
- SSR：自由度为$p - 1$

因而，自由度的分解为：
$$
n - 1 = (n - p) + (p - 1)
$$

**课堂思考: 假设模型有两个解释变量，其中$x_{1}$是连续变量，$x_{2}$是包含5个分类的分类变量，SSR的自由度为多少？**

## 方差分析表

变异来源 | 平方和  | 自由度  | 均方 |
---- | ---- | ----- | --- |
回归模型 | SSR | $p - 1$ | MSR = SSR/$(p-1)$ |
误差 | SSE | $n - p$ | MSE = SSE/$(n-p)$ |
总变异 | SST | $n-1$ | MST = SST/$(n-1)$

Table:多元线性回归的方差分析表

相应地，可以构造$F$检验：
$$
F(\text{df}_{\text{SSR}}, \text{df}_{\text{SSE}}) = \frac{\text{MSR}}{\text{MSE}} ?> F_{\alpha}
$$

**延伸内容：聚类分析**

## 模型选择

- 模型选择：**精确性原则** vs **简约性原则**
- 情境：假定在线性回归模型$A$的基础上，加了几个变量得到模型$B$，应当如何在模型A和B之间选择？

构造$F$检验：
$$
F(\Delta \text{df}, \text{df}_{\text{SSE}}) = \frac{\Delta \text{SSR} / \Delta \text{df}}{\text{MSE}_{\text{B}}} ?> F_{\alpha}
$$

# 线性回归案例

## 中老年精神健康案例

从CHARLS数据中随机抽取样本$n = 488$，考虑中老年抑郁水平。`income`为个人收入，以万元计；`educ`表示教育水平是否在初中及以上，`hukou`表示是否是城市户口。

```{r}

rm(list = ls())
suppressMessages(library(dplyr))
suppressMessages(library(ggplot2))
load("charlswh.RData")
charlswh <- charlswh %>%
    rename(hukou = r4hukou) %>%
    filter(hukou < 2) %>%
    mutate(income = income / 10000)

```

变量 | 均值 | 标准差 | 最小值 | 最大值 |
--- | --- | --- | --- | --- |
cesd10 | `r mean(charlswh$cesd10)` | `r sd(charlswh$cesd10)` | `r min(charlswh$cesd10)` | `r max(charlswh$cesd10)` |
income | `r mean(charlswh$income)` | `r sd(charlswh$income)` | `r min(charlswh$income)` | `r max(charlswh$income)` |
educ | `r mean(charlswh$educ)` | `r sd(charlswh$educ)` | `r min(charlswh$educ)` | `r max(charlswh$educ)` |
hukou | `r mean(charlswh$hukou)` | `r sd(charlswh$hukou)` | `r min(charlswh$hukou)` | `r max(charlswh$hukou)` |

Table: 描述性统计量

## 收入与精神健康

```{r}

# estimate three models
fit1 <- lm(cesd10 ~ income, data = charlswh)
fit2 <- lm(cesd10 ~ income + educ, data = charlswh)
fit3 <- lm(cesd10 ~ income + educ + hukou, charlswh)

```

变量   | 模型1 | 模型2 | 模型3 |
---- | :-----: |:---: |:-----: |
常数项 | `r summary(fit1)$coef[1,1]` (`r summary(fit1)$coef[1,2]`) | `r summary(fit2)$coef[1,1]` (`r summary(fit2)$coef[1,2]`) | `r summary(fit3)$coef[1,1]` (`r summary(fit3)$coef[1,2]`) |
income | `r summary(fit1)$coef[2,1]` (`r summary(fit1)$coef[2,2]`) | `r summary(fit2)$coef[2,1]` (`r summary(fit2)$coef[2,2]`) | `r summary(fit3)$coef[2,1]` (`r summary(fit3)$coef[2,2]`) |
educ | - | `r summary(fit2)$coef[3,1]` (`r summary(fit2)$coef[3,2]`) | `r summary(fit3)$coef[3,1]` (`r summary(fit3)$coef[3,2]`) |
hukou | - | - | `r summary(fit3)$coef[4,1]`$^{\text{ns}}$ (`r summary(fit3)$coef[4,2]`) |
$R^{2}$ | `r summary(fit1)$r.squared` |`r summary(fit2)$r.squared` | `r summary(fit3)$r.squared` |

Table: 不同线性回归模型比较

**课堂讨论：应选择哪个模型？**

## 变异分解与模型选择

```{r}

summary.aov(fit3)

```

## 最终模型

权衡**精确性原则**与**简约性原则**，选择模型2。

![模型估计结果](figures/fit2.png){width=75%}

## 显著性与效应大小

- 统计显著性（statistical significance）
- 效应大小（effect size）


**课堂讨论：二者有何区别？**

## 变异分解：编程计算

```{r, echo = T}

# calculate predicted values
yhat <- predict.lm(fit2)
# calculate and print SST, SSR, and SSE
ybar <- mean(charlswh$cesd10)
sst <- sum((charlswh$cesd10 - ybar) ^ 2)
ssr <- sum((yhat - ybar) ^ 2)
sse <- sum((charlswh$cesd10 - yhat) ^ 2)
c(sst, ssr, sse)

```

## 变异分解：系统输出

```{r, echo = T}

# variation decomposition
summary.aov(fit2)

```

# 线性回归诊断

## 因变量分布

```{r, fig.height=4}

suppressMessages(library(dplyr))
suppressMessages(library(ggplot2))
suppressMessages(library(e1071))

# plot
ggplot(charlswh, aes(x = cesd10)) + geom_histogram(bins = 50) + theme_bw()

```

## Box-Cox变换

**课堂思考: (1) 对数变换是否合适？(2) 如何推导出“变化比例”这一含义？**

## 多重共线性

参数估计值
$$
\hat{\beta} = (X'X)^{-1}X'y
$$
要求$X'X$是**可逆（非奇异）**的。

- 完全多重共线性：模型无法识别
- 严重多重共线性：不影响估计的无偏性和一致性，损害参数估计的**有效性**，及标准误会增大
- 判断标准：**方差膨胀因子**（variance inflation factor, VIF）最大值超过10，平均值明显大于1

## 消除共线性

- $k$水平分类变量：虚拟变量（dummy variable）化后，只能有$k-1$个虚拟变量
- 减少解释变量个数
- 维度规约：因子分析
- 变量选择：如lasso等统计机器学习方法，尤其是$n < p$时模型无法识别的情形

```{r, echo = T}

suppressMessages(library(car))
# calculate VIF
vif(fit2)

```

## 残差分布

## 纳入二次项

- 处理$U$型关系

## 纳入交互项（调节作用）

## 异常值处理


## 实践中的回归假设

- 正交假设 （\textcolor{red}{需要检验}）

# 线性回归高阶议题（*）

## 内生性

## 匹配法（matching）

## 随机控制试验法（RCT）

## 异质性

## 分层线性模型

## 贝叶斯视角

背景：

- Efron提出的bootstrapping方法
- 大数据时代的统计推断

案例思考：

- 射击选手B，999/1000
- 射击选手A，100/100

## 案例思考

![灵犀一指 (999/1000) vs 小李飞刀 (100/100)](figures/bayesian.jpeg){width=60%}

## 先验的作用

情境一（先验8/10）：

- A：先验(8/10) + 数据（999/1000）-> 后验（1007/1010 = 0.9997） [**获胜**]
- B: 先验(8/10) + 数据（100/100）-> 后验（108/110 = 0.9818）


情境二（先验9999/10000）：

- A：先验(9999/10000) + 数据（999/1000）-> 后验（10998/11000 = 0.9998）
- B: 先验(9999/10000) + 数据（100/100）-> 后验（10099/10100 = 0.9999）[**获胜**]


## 回归分析总结
